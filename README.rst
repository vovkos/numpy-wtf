Вот цефак
=========

Привет Толик!

Набросал на днях простенький двухслойный классификатор по статье Нильсена (http://neuralnetworksanddeeplearning.com/). Всё казалось просто, статью отложил, выводил формулки из головы, писал тоже из головы, не сверяясь с исходной репой Нильсена (за исключением сериализации самой выборки для обучения). Стал сверять результаты -- не сошлось (ессно). После постепенного приведения одного к другому наткнулся на необъяснимый эффект:

Объявляя исходные матрицы как:

.. code:: python

	weight_1 = np.zeros((hidden_neuron_count, input_neuron_count))

	# test tabs
		# test tabs
			# test tabs
		
	
и

.. code:: python

	weight_1 = np.zeros((input_neuron_count, hidden_neuron_count)).transpose()

мы получаем разные результаты, причём не сразу, а начинается расхождение где-то на тысячной итерации. Сидел в Pycharm, сверял форму и типы -- всё совпадает (вроде). А выход разный. Вот, собственно, цефак? Ну бред же! Или я что-то очевидное упускаю?
